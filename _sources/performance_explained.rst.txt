Performance Metrics
===================

This section documents the key data files used in the job analysis pipeline and the performance metrics obtained from our analysis.

Job Analysis Export CSV
-----------------------

The ``job_analysis_export.csv`` file contains the processed job listings with extracted incentives and classifications. 
This file is generated by running the pipeline and serves as the primary output for analysis.

Key columns include:
- Job metadata (title, URL, portal name, date)
- Location information (city, state, country)
- Company details (name, size)
- Employment attributes (job type, time model, position level)
- Binary indicators for 20+ incentive categories

Job Analysis SQLite-Database
----------------------------

The ``job_analysis_shorter.db`` file is an SQLite-Database that stores all processed job listings. 
It maintains the same schema as the CSV export but allows for more efficient querying and analysis.

The database uses a composite unique constraint on (Job_URL, Stadt, Zeitmodell, Position) to prevent 
duplicate entries while allowing the same job to appear with different location/work model combinations.
Click :ref:`here<SQlite Module>` to read more details about the SQlite-Database.

Performance
-----------

The performance_history.csv file documents accuracy and recall metrics across five independent evaluation runs. 
Results are reported separately for incentives and non-incentives, with each section providing both combined and 
category-specific accuracy and recall. Additionally, overall accuracy and recall are aggregated across all categories 
to provide a comprehensive performance overview.

1. Accuracy
-----------

The average accuracy of all categories
over five runs is 0.8286.

Find the individual values below.

.. figure:: results/accuracy.png
   :width: 400
   :alt: Alternative text
   
   Accuracy per category and average accuracy

2. Recall
---------

The average recall of all categories
the five runs is 0.9076.

Find the individual values below.

.. figure:: results/recall.png
   :width: 400
   :alt: Alternative text
   
   Recall per category and average recall


The processing of one job took ~35s (depending on the length of the content)

Quantitative benchmarks reveal consistent metrics across most categories, with statistically significant deviations 
observed in flexible Arbeitszeiten, Weiterbildung, and Arbeitsausstattung. These outliers exhibit lower recall 
and accuracy due to persistent lexical hallucination tendencies in the LLMâ€™s output space. Targeted prompt engineering 
interventions (see :ref:`here<III. Incentives Extraction>`) partially mitigated errors but failed 
to resolve systemic semantic drift.

Architectural limitations-specifically, hardware-induced model scaling constraints-preclude deploying larger LLMs that 
could address this via expanded context windows. Performance remains Pareto-optimal given current VRAM ceilings.

For more information on that look :ref:`here<III. Incentives Extraction>`.

3. Branche
----------

The branche_test.py evaluation addresses limitations in the original IT-only test data by introducing a structured benchmark 
with 4 jobs per category (2 English, 2 German) from branche_test.csv. I use ``branche_test.py`` for testing, which isolates 
the Llama-3.2-3B-Instruct model's branch classification capability using the same prompt 
engineering and inference parameters as :ref:`extraction.py<Extraction Module>`.

The metrics for the branche:

* Accuracy: 1.00
* Recall: 1.00


Used Hardware/Software
----------------------

* GPU: NVIDIA GeForce GTX 1070 with 8GB VRAM
* CPU: Intel(R) Core(TM) i7-6700K CPU @ 4.00GHz
* RAM: 16 GB
* System: Debian GNU/Linux 12
* Python-Version: 3.11


